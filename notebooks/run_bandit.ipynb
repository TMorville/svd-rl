{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose of this notebook is to implement a simple linear contextual bandit from the TF research libary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*We create a recommendation system bandit problem as follows. The Jester Dataset (Goldberg et al., 2001) provides continuous ratings in [-10, 10] for 100 jokes from a total of 73421 users. We find a complete subset of n = 19181 users rating all 40 jokes. Following Riquelme et al. (2017), we take d = 32 of the ratings as the context of the user, and k = 8 as the arms. The agent recommends one joke, and obtains the reward corresponding to the rating of the user for the selected joke.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/usr/local/anaconda3/envs/tf/lib/python3.6/site-packages/ipykernel_launcher.py']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "\n",
    "from linear_bandit.sample_jester_data import sample_jester_data\n",
    "from linear_bandit.sample_retail_data import sample_retail_data\n",
    "\n",
    "from linear_bandit.contextual_bandit import run_contextual_bandit\n",
    "\n",
    "from linear_bandit.linear_full_posterior_sampling import LinearFullPosteriorSampling\n",
    "from linear_bandit.neural_bandit_model import NeuralBanditModel\n",
    "from linear_bandit.neural_linear_sampling import NeuralLinearPosteriorSampling\n",
    "\n",
    "from linear_bandit.bandit_algorithm import BanditAlgorithm\n",
    "from linear_bandit.contextual_bandit import ContextualBandit\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_route = '/Users/tmo/Data/bandits/'\n",
    "\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "FLAGS.set_default('alsologtostderr', True)\n",
    "flags.DEFINE_string('logdir', data_route + 'logs/', 'Base directory to save output')\n",
    "\n",
    "FLAGS(sys.argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_vals = get_jester_data(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, opt_rewards, opt_actions, num_actions, context_dim = sampled_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 40)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams_linear = tf.contrib.training.HParams(num_actions=num_actions, \n",
    "                                             context_dim=context_dim, \n",
    "                                             a0=6,\n",
    "                                             b0=6,\n",
    "                                             lambda_prior=0.25,\n",
    "                                             initial_pulls=2)\n",
    "\n",
    "linear_bandit = LinearFullPosteriorSampling(name='linear_bandit', hparams=hparams_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams_nlinear = tf.contrib.training.HParams(num_actions=num_actions,\n",
    "                                                context_dim=context_dim,\n",
    "                                                init_scale=0.3,\n",
    "                                                activation=tf.nn.relu,\n",
    "                                                layer_sizes=[50],\n",
    "                                                batch_size=512,\n",
    "                                                activate_decay=True,\n",
    "                                                initial_lr=0.1,\n",
    "                                                max_grad_norm=5.0,\n",
    "                                                show_training=False,\n",
    "                                                freq_summary=1000,\n",
    "                                                buffer_s=-1,\n",
    "                                                initial_pulls=2,\n",
    "                                                reset_lr=True,\n",
    "                                                lr_decay_rate=0.5,\n",
    "                                                training_freq=1,\n",
    "                                                training_freq_network=50,\n",
    "                                                training_epochs=100,\n",
    "                                                a0=6,\n",
    "                                                b0=6,\n",
    "                                                lambda_prior=0.25, \n",
    "                                              verbose=False)\n",
    "\n",
    "\n",
    "neural_bandit = NeuralLinearPosteriorSampling('neural_bandit', hparams_nlinear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bandit(model, hparams, plot=True, plot_freq=500):\n",
    "        \n",
    "    num_contexts = dataset.shape[0]\n",
    "    \n",
    "    h_actions = []\n",
    "    h_rewards = []\n",
    "    \n",
    "    # Run the contextual bandit process\n",
    "    for i in range(num_contexts):\n",
    "        context = dataset[i, :context_dim] # Grab the ith line up until joke 32\n",
    "        action = model.action(context) # Just one model with an action for the context\n",
    "        reward = dataset[i, context_dim+action] # Grab the reward from the 8 possible rewards\n",
    "\n",
    "        model.update(context, action, reward)\n",
    "\n",
    "        h_actions.append(action)\n",
    "        h_rewards.append(reward)\n",
    "        \n",
    "        if plot and model.t % plot_freq == 0:\n",
    "            optimal_action_frequencies = [[elt, list(opt_actions).count(elt)] for elt in set(opt_actions)]\n",
    "            model_action_frequencies = [[elt, list(h_actions).count(elt)] for elt in set(h_actions)]\n",
    "            \n",
    "            plot_optimal_model_actions(optimal_action_frequencies, \n",
    "                                       model_action_frequencies, \n",
    "                                       model.t)\n",
    "            \n",
    "        \n",
    "    print('Optimal total reward = {}.'.format(np.sum(opt_rewards)))\n",
    "    print('Total reward from bandit = {}.'.format(np.sum(h_rewards)))\n",
    "    print('Reward ratio = {}'.format(np.sum(h_rewards)/np.sum(opt_rewards)))\n",
    "        \n",
    "    optimal_action_frequencies = [[elt, list(opt_actions).count(elt)] for elt in set(opt_actions)]\n",
    "    model_action_frequencies = [[elt, list(h_actions).count(elt)] for elt in set(h_actions)]\n",
    "    \n",
    "    return optimal_action_frequencies, model_action_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal total reward = 11341.560000000001.\n",
      "Total reward from bandit = 4533.889999999999.\n",
      "Reward ratio = 0.39975893968730924\n",
      "Optimal total reward = 11341.560000000001.\n",
      "Total reward from bandit = 4735.59.\n",
      "Reward ratio = 0.4175430893104652\n",
      "Optimal total reward = 11341.560000000001.\n",
      "Total reward from bandit = 5065.33.\n",
      "Reward ratio = 0.44661669117828584\n",
      "Optimal total reward = 11341.560000000001.\n",
      "Total reward from bandit = 5061.74.\n",
      "Reward ratio = 0.44630015623952957\n",
      "Optimal total reward = 11341.560000000001.\n",
      "Total reward from bandit = 5047.9400000000005.\n",
      "Reward ratio = 0.4450833924080991\n",
      "Optimal total reward = 11341.560000000001.\n",
      "Total reward from bandit = 5270.5599999999995.\n",
      "Reward ratio = 0.46471208546266995\n",
      "Optimal total reward = 11341.560000000001.\n",
      "Total reward from bandit = 5295.6.\n",
      "Reward ratio = 0.46691989461767164\n",
      "Optimal total reward = 11341.560000000001.\n",
      "Total reward from bandit = 5188.59.\n",
      "Reward ratio = 0.4574846846465565\n",
      "19.2 s ± 3.42 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "oaf, maf = run_bandit(linear_bandit, hparams_linear, plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
